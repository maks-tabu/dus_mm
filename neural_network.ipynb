{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "91e9605d",
    "outputId": "5f0d650c-8db6-4346-e06c-15b8d63e4783"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#os.chdir('/content/drive/MyDrive/Github/dus_mm/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import glob\n",
    "from typing import List, Callable, Optional\n",
    "from functools import reduce\n",
    "os.chdir('data2')\n",
    "\n",
    "columnsX = ['freq','AX','BX','BY','CY','CX','DX','DY','AY']\n",
    "columnsY = ['AX','BX','BY','CY','CX','DX','DY','AY','AQ','AL','BQ','BL','CQ','CL','DQ','DL']\n",
    "columnsYout = ['AQ','AL','BQ','BL','CQ','CL','DQ','DL','X','Y','Q','L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1a3f18f9"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "numb_files = len(glob.glob('data_output*'))\n",
    "for i in glob.glob('data_output*'): \n",
    "    numb = i[11]\n",
    "    if (numb == 'data_output.txt'):\n",
    "        numb = ''\n",
    "    y = pd.concat([y,pd.read_table(os.path.join('data_output%s.txt' % numb), header = 0, names = columnsY, \n",
    "                                   index_col = False, sep='\\s+', engine='python', dtype ='float64')], ignore_index = 1)\n",
    "    X = pd.concat([X,pd.read_table(os.path.join('data_input%s.txt' % numb), header = 0, names = columnsX, \n",
    "                                   index_col = False, sep='\\s+', engine='python', dtype ='float64')], ignore_index = 1) \n",
    "    \n",
    "nelem = 64\n",
    "freq_bias = [252.92, 97.13, 87.06, 57.57, 12.55, 3.40, -4.78, -11.92]\n",
    "#freq_bias = [9.8,   4.9,   1.03, -12.05]\n",
    "drop_ind = y.loc[(y > nelem).any(axis = 1)].index\n",
    "\n",
    "X = utils.init_in(X, freq_bias)\n",
    "# y = utils.decomposition(y)\n",
    "\n",
    "# drop_ind.append(y.loc[(y == 0).all(axis = 1)].index)\n",
    "# X = X.drop(drop_ind).reset_index(drop=True)\n",
    "# y = y.drop(drop_ind).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq1</th>\n",
       "      <th>phaseA1</th>\n",
       "      <th>phaseB1</th>\n",
       "      <th>phaseC1</th>\n",
       "      <th>phaseD1</th>\n",
       "      <th>AXBX1</th>\n",
       "      <th>BYCY1</th>\n",
       "      <th>CXDX1</th>\n",
       "      <th>DYAY1</th>\n",
       "      <th>freq2</th>\n",
       "      <th>...</th>\n",
       "      <th>DYAY7</th>\n",
       "      <th>freq8</th>\n",
       "      <th>phaseA8</th>\n",
       "      <th>phaseB8</th>\n",
       "      <th>phaseC8</th>\n",
       "      <th>phaseD8</th>\n",
       "      <th>AXBX8</th>\n",
       "      <th>BYCY8</th>\n",
       "      <th>CXDX8</th>\n",
       "      <th>DYAY8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.884</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.748</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.480</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.907</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.633</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.617</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.324</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.700</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.914</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.119</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>0.595</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>0.525</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.698</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.229</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.973</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.282</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2151 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        freq1  phaseA1  phaseB1  phaseC1  phaseD1  AXBX1  BYCY1  CXDX1  DYAY1  \\\n",
       "number                                                                          \n",
       "0       0.088    0.997    0.000    0.998    0.000  0.952  1.000  0.872  0.834   \n",
       "1       0.000    1.000    0.000    0.999    0.000  0.913  0.884  1.000  0.975   \n",
       "2       0.480    1.000    0.001    1.000    0.001  0.994  0.899  0.907  1.000   \n",
       "3       0.617    1.000    0.000    1.000    0.000  0.984  0.954  0.955  1.000   \n",
       "4       0.700    1.000    0.001    1.000    0.001  0.925  1.000  0.980  0.914   \n",
       "...       ...      ...      ...      ...      ...    ...    ...    ...    ...   \n",
       "2146    0.595    1.000    0.000    0.999    0.002  0.871  0.901  1.000  0.904   \n",
       "2147    0.525    1.000    0.000    1.000    0.000  0.969  0.946  1.000  0.974   \n",
       "2148    1.000    1.000    0.001    1.000    0.000  0.987  0.932  0.974  1.000   \n",
       "2149    0.000    0.999    0.000    0.998    0.000  0.973  1.000  0.877  0.867   \n",
       "2150    0.000    0.999    0.000    0.998    0.000  0.822  0.892  1.000  0.921   \n",
       "\n",
       "        freq2  ...  DYAY7  freq8  phaseA8  phaseB8  phaseC8  phaseD8  AXBX8  \\\n",
       "number         ...                                                            \n",
       "0       0.986  ...  0.030  0.504    0.681    0.416    0.789    0.515  0.141   \n",
       "1       0.748  ...  1.000  0.069    0.879    0.186    0.421    0.888  0.304   \n",
       "2       0.739  ...  0.486  0.744    0.371    0.330    0.973    0.771  0.055   \n",
       "3       0.097  ...  0.513  0.150    0.964    0.827    0.996    0.724  0.324   \n",
       "4       1.000  ...  0.283  0.190    0.767    0.186    0.978    0.363  0.262   \n",
       "...       ...  ...    ...    ...      ...      ...      ...      ...    ...   \n",
       "2146    0.690  ...  0.619  0.481    0.083    0.082    0.996    0.691  0.491   \n",
       "2147    0.698  ...  1.000  0.000    0.213    0.018    0.688    0.952  0.782   \n",
       "2148    0.989  ...  0.366  0.654    0.857    0.227    0.268    0.680  0.229   \n",
       "2149    1.000  ...  0.287  0.156    0.574    0.020    0.698    0.219  0.057   \n",
       "2150    0.780  ...  0.007  0.573    0.727    0.052    0.180    0.999  1.000   \n",
       "\n",
       "        BYCY8  CXDX8  DYAY8  \n",
       "number                       \n",
       "0       0.006  0.011  1.000  \n",
       "1       1.000  0.056  0.111  \n",
       "2       0.633  1.000  0.231  \n",
       "3       1.000  0.831  0.187  \n",
       "4       0.085  0.119  1.000  \n",
       "...       ...    ...    ...  \n",
       "2146    0.201  0.160  1.000  \n",
       "2147    1.000  0.142  0.212  \n",
       "2148    1.000  0.196  0.052  \n",
       "2149    0.046  0.282  1.000  \n",
       "2150    0.595  0.168  0.185  \n",
       "\n",
       "[2151 rows x 72 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y-0L1UGgHT6"
   },
   "outputs": [],
   "source": [
    "alfa = 1\n",
    "koef = np.zeros(len(freq_bias))\n",
    "X1 = utils.init_in(X.copy(), freq_bias)\n",
    "for i in range (100):\n",
    "    for n in range(len(freq_bias)):\n",
    "        koef[n] = (0.5 - X1['freq' + str(n+1)].mean())\n",
    "    freq_bias = freq_bias + koef * alfa\n",
    "    X1 = utils.init_in(X.copy(), freq_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction, y, diff: bool = False):\n",
    "#     if (loss_funtion == 'mean_squared_error'):\n",
    "    if (diff == True):\n",
    "        loss = prediction - y\n",
    "    else:\n",
    "        loss = np.sum(np.sum(np.power((prediction - y), 2), axis = None) )/ (2 * y.shape[0])\n",
    "#     elif (loss_funtion == 'loglos'):\n",
    "#         if (diff == True):\n",
    "#             loss = (prediction - y)/((1 - prediction) * prediction)\n",
    "#         else:\n",
    "#             loss = - np.sum(y * np.log(prediction) + (1 - y) * np.log(1 - prediction))/y.shape[0]     \n",
    "    return loss\n",
    "\n",
    "def activation(name: str, x: np.ndarray, diff: bool = False):\n",
    "        if (name == 'relu'):\n",
    "            if (diff == True):\n",
    "                x[x >= 0] = 1\n",
    "                x[x < 0] = 0\n",
    "            else:\n",
    "                x[x < 0] = 0\n",
    "        elif (name == 'linear'):\n",
    "            if (diff == True):\n",
    "                x = 1\n",
    "            else:\n",
    "                x = x\n",
    "        elif (name == 'sigmoid'):\n",
    "            if (diff == True):\n",
    "                x = activation(name, x) * (1 - activation(name, x))\n",
    "            else:\n",
    "                x = 1/(1 + np.exp(-x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "63666d57"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, previous_layer,  neurons: int, activation_name: str = 'linear') -> None:\n",
    "        self.previous_layer = previous_layer\n",
    "        self.next_layer = None\n",
    "        self.neurons: int = neurons\n",
    "        self.activation_name = activation_name\n",
    "        self.__weights: np.ndarray \n",
    "        self.weights_cache = self.initialization_weights()\n",
    "        self.outputs_cache: np.ndarray\n",
    "        self.deltas_cache: np.ndarray \n",
    "        self.gradients_cache: np.ndarray\n",
    "    \n",
    "    def initialization_weights(self) -> np.ndarray:\n",
    "        if self.previous_layer is None:\n",
    "            self.weights_cache = np.array([]) \n",
    "        else:\n",
    "            gamma = np.sqrt(6) / np.sqrt(self.previous_layer.neurons + self.neurons)\n",
    "            self.weights_cache = np.random.rand(self.neurons,self.previous_layer.neurons + 1) * 2 * gamma - gamma          \n",
    "            return self.weights_cache\n",
    "    \n",
    "    def calculate_outputs(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        if self.previous_layer is None:\n",
    "            self.outputs_cache = inputs\n",
    "        else:\n",
    "            inputs = np.concatenate((np.ones([inputs.shape[0],1]), inputs), axis = 1)\n",
    "            self.outputs_cache =  activation(self.activation_name, inputs @ self.weights_cache.T)       \n",
    "        return self.outputs_cache\n",
    "    \n",
    "    def calculate_deltas_and_gradients(self, y: np.ndarray) -> None:\n",
    "        inputs = self.previous_layer.outputs_cache\n",
    "        inputs = np.concatenate((np.ones([inputs.shape[0],1]), inputs), axis = 1)\n",
    "        if self.next_layer is None:\n",
    "            self.deltas_cache = compute_loss(self.outputs_cache, y, diff = True) * activation(self.activation_name, np.dot(inputs, self.weights_cache.T) , diff = True)\n",
    "        else:\n",
    "            self.deltas_cache = np.dot(self.next_layer.deltas_cache, self.next_layer.weights_cache[:,1:]) * activation(self.activation_name, np.dot(inputs, self.weights_cache.T), diff = True)\n",
    "        self.gradients_cache = (1 / y.shape[0]) * np.dot(self.deltas_cache.T, inputs)\n",
    "    \n",
    "    def _record_weights(self):\n",
    "        self.__weights = self.weights_cache\n",
    "    \n",
    "    def _playback_weights(self):\n",
    "        self.weights_cache = self.__weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e26a390"
   },
   "outputs": [],
   "source": [
    "class NN_model: \n",
    "    def __init__(self, input_dimension: int, loss = 'mean_squared_error'):\n",
    "        self.sum_neurons = 0\n",
    "        self.layers: List[Layer] = [] \n",
    "        self.add_layer(input_dimension)\n",
    "        self.loss = loss\n",
    "        self.optimizer = 'adam' #'adam'\n",
    "        self.max_epoch = 1000\n",
    "        self.chunk_size = 32\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99\n",
    "        self.epsilon = 1e-8\n",
    "        self.learning_rate = 0.01\n",
    "        self.regularization = None #'l2'\n",
    "        self.best_loss_val = 1e10\n",
    "    \n",
    "    def add_layer(self, neurons: int, activation) -> None:\n",
    "        if not self.layers:\n",
    "            new_layer = Layer(None, neurons)\n",
    "        else:\n",
    "            new_layer = Layer(self.layers[-1], neurons, activation)\n",
    "            self.layers[-1].next_layer = new_layer   \n",
    "        self.layers.append(new_layer)\n",
    "        self.sum_neurons += neurons \n",
    "    \n",
    "    def calculate_outputs(self, inputs:  np.ndarray) -> np.ndarray:\n",
    "         return reduce(lambda inputs, layer: layer.calculate_outputs(inputs), self.layers, inputs)  \n",
    "    \n",
    "    def backpropagate(self, y):\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            self.layers[i].calculate_deltas_and_gradients(y)          \n",
    "\n",
    "    def gradient_checking(self, x, y) -> None:\n",
    "        e: float = 1e-4\n",
    "        numgrad = np.array([])\n",
    "        grad = np.array([])\n",
    "        self.calculate_outputs(x)\n",
    "        self.backpropagate(y)\n",
    "        j = 0\n",
    "        for i in range(1, len(self.layers)):\n",
    "            grad = np.concatenate([grad, self.layers[i].gradients_cache.ravel()])\n",
    "            numgrad = np.concatenate([numgrad, np.zeros(self.layers[i].weights.size)])\n",
    "            perturb = np.diag(e * np.ones(self.layers[i].weights.size))\n",
    "            weights = self.layers[i].weights.copy()\n",
    "            for k in range(self.layers[i].weights.size):\n",
    "                self.layers[i].weights += perturb[:,k].reshape(self.layers[i].weights.shape)\n",
    "                predicted_plus  = self.calculate_outputs(x)\n",
    "                self.layers[i].weights -= perturb[:,k].reshape(self.layers[i].weights.shape)\n",
    "                predicted_minus = self.calculate_outputs(x)\n",
    "                numgrad[j] = (compute_loss(predicted_plus, y) - compute_loss(predicted_minus, y))/(2 * e)\n",
    "                j += 1\n",
    "            self.layers[i]._playback_weights(self):\n",
    "        diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "        print('Relative Difference: %g' % diff) \n",
    "     \n",
    "#     def optimize(self, X_train, y_train, X_val, y_val, lambda_new = 0 , theta_new = None):\n",
    "#         if (theta_new is None):\n",
    "#             theta_new = self.theta\n",
    "#         for epoch in range(self.max_epoch):\n",
    "#             m = np.zeros(self.theta.shape)\n",
    "#             v = np.zeros(self.theta.shape)\n",
    "#             lst =  np.arange(y_train.shape[0])\n",
    "#             np.random.shuffle(lst)\n",
    "#             chunks = np.array([lst[i: i + self.chunk_size] for i in range(0, y_train.shape[0], self.chunk_size)])\n",
    "#             for t, chunk in enumerate(chunks):\n",
    "#                 xi = X_train[chunk]\n",
    "#                 yi = y_train[chunk]\n",
    "#                 self.calculate_outputs(xi)\n",
    "#                 self.backpropagate(yi)\n",
    "#                 if (self.optimizer == 'adam'):\n",
    "#                     for i in range(1, len(self.layers)):\n",
    "#                         m = self.beta_1 * m + (1 - self.beta_1) * layer[i].calculate_gradients()\n",
    "#                         v = self.beta_2 * v + (1 - self.beta_2) * np.power(grad, 2)\n",
    "#                         m_hat = m / (1 - np.power(self.beta_1, t + 1))\n",
    "#                         v_hat = v / (1 - np.power(self.beta_2, t + 1))\n",
    "#                         layers[i].weights = layers[i].weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon) \n",
    "#                 elif (self.optimizer == 'gradient_descent'):\n",
    "#                     theta_new = theta_new - self.learning_rate * grad\n",
    "#             a_out_train = self.feedforward(X_train, theta_new)\n",
    "  \n",
    "#             loss_train = self.compute_loss(a_out_train, y_train, theta_new)# + lambda_new * np.sum(np.power(self.nulling(theta_new), 2))/2\n",
    "#             a_out_val = self.feedforward(X_val, theta_new)\n",
    "#             loss_val = self.compute_loss(a_out_val, y_val, theta_new) #+ lambda_new * np.sum(np.power(self.nulling(theta_new), 2))/2\n",
    "            \n",
    "#             if (self.best_loss_val > loss_val):  \n",
    "#                 self.theta = theta_new\n",
    "#                 self.best_loss_val = loss_val        \n",
    "                \n",
    "#             print(f'{np.round(loss_train,2)}    {np.round(loss_val,2)}    {epoch}')\n",
    "#         return loss_train, loss_val\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return str(len(self.sequential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGYK1b3NR6-q"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'loglos')\n",
    "dus_mm.add_layer(16, 'sigmoid')\n",
    "dus_mm.add_layer(16, 'sigmoid')\n",
    "dus_mm.add_layer(y.shape[1], 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.calculate_outputs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.backpropagate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.gradient_checking(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dus_mm.layers[2].output_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh_MQJ_MR3Hp"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('mnist.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "y[y == 10] = 0\n",
    "y0 = np.zeros((y.size, 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y0[i,y[i]] = 1\n",
    "y = y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa38a8e6"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1])\n",
    "dus_mm.add(Layer(128, 'relu'))\n",
    "dus_mm.add(Layer(128, 'relu'))\n",
    "dus_mm.add(Layer(y.shape[1], 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFpD5CtzZlxA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# накидываем тысячу точек от -3 до 3\n",
    "X = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "\n",
    "# задаём линейную функцию, которую попробуем приблизить нашей нейронной сетью\n",
    "def f(x):    \n",
    "    return 2 * np.sin(x) + 5\n",
    "\n",
    "f = np.vectorize(f)\n",
    "\n",
    "# вычисляем вектор значений функции\n",
    "y = f(X)\n",
    "\n",
    "# отрисовываем результат приближения нейросетью поверх исходной функции\n",
    "plt.scatter(X_train, y, color='black', antialiased=True)\n",
    "#plt.plot(, model.predict(x), color='magenta', linewidth=2, antialiased=True)\n",
    "plt.show()\n",
    "\n",
    "# выводим веса на экран\n",
    "# for layer in model.layers:\n",
    "#     weights = layer.get_weights()\n",
    "#     print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XlZMY-XBaHP5"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'mean_squared_error')\n",
    "dus_mm.add(Layer(y.shape[1], 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1624464853613,
     "user": {
      "displayName": "Maxim Tabolin",
      "photoUrl": "",
      "userId": "11591994697218843812"
     },
     "user_tz": -180
    },
    "id": "jOpGhdRhNZe4",
    "outputId": "f4c275cd-736e-4c7c-d5bb-a7c15818c4e2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "lambda_vec = [0]\n",
    "\n",
    "nfold = 5\n",
    "slice_fold = int(y.shape[0] // nfold)\n",
    "lst =  np.arange(y.shape[:(5 * slice_fold)][0])\n",
    "np.random.shuffle(lst)\n",
    "list_fold = np.array([lst[i * slice_fold:(i + 1) * slice_fold] for i in range(nfold)])\n",
    "\n",
    "train_error = np.zeros(nfold)\n",
    "val_error = np.zeros(nfold)\n",
    "train_reg = np.zeros(len(lambda_vec))\n",
    "val_reg = np.zeros(len(lambda_vec))\n",
    "theta_new = dus_mm.theta.copy()\n",
    "\n",
    "for k in range(len(lambda_vec)):\n",
    "    theta_new\n",
    "    for i in range(nfold):\n",
    "        X_train = X[np.delete(list_fold, i, 0).ravel()]\n",
    "        y_train = y[np.delete(list_fold, i, 0).ravel()]\n",
    "        X_val = X[list_fold[i]]\n",
    "        y_val = y[list_fold[i]]\n",
    "        train_error[i], val_error[i] = dus_mm.optimize(X_train, y_train, X_val, y_val, lambda_vec[k], theta_new)\n",
    "    train_reg[k] = np.mean(train_error)  \n",
    "    val_reg[k] = np.mean(val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S7sRgeqgHUZ"
   },
   "source": [
    "['AQ','AL','BQ','BL','CQ','CL','DQ','DL','X','Y','Q','L']\n",
    "  -1., -0., -2.,  1., 34., 65.,  5., 23., 81., -2., 18., 21\n",
    "   2.,  1.,  1.,  0., 15., 65., 24.,  2., 83.,  1.,  7.,  4.\n",
    "   0.,  1., -1.,  10.,21., 63., 26.,  6., 86.,  -1., 12.,-20.\n",
    "      \n",
    "   1.,  1.,  3.,  7., 47., 15., 20.,  48., 54.,  3.,-11., 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqd2hAFPgHUa"
   },
   "outputs": [],
   "source": [
    "XX = pd.read_table(os.path.join('data7A-1_2.txt'), header = 0, names = columnsX, index_col = False, sep='\\s+', engine='python', dtype =\"float\")\n",
    "XX = utils.init_in(XX,freq_bias)\n",
    "yy = np.round(dus_mm.feedforward(np.array(XX))[:12])\n",
    "#yy = np.abs(pd.DataFrame(yy.astype('float'), columns = columnsYout))\n",
    "#yy = utils.integration(yy,nelem)\n",
    "# predictions = NN_model.predict(XX)\n",
    "# yy = np.abs(pd.DataFrame(predictions.astype('float'), columns = columnsYout))\n",
    "#yy = utils.integration(yy,0.72*1.33,1.33) # перемычки 30 мкм вместо 40 мкм 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB-t23aygHUc",
    "outputId": "c4f73a2e-a70c-4d9f-b05a-b5ea30d0823a"
   },
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF3fAeAjgHUd"
   },
   "outputs": [],
   "source": [
    "len(columnsYout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-MlF9J-gHUe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(yy.astype('float'), col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN3uHI8NgHUf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yy = utils.integration(np.round(dus_mm.feedforward(np.array(XX))[:12]),nmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eto14XwfoaCJ"
   },
   "outputs": [],
   "source": [
    "a_out_train = self.feedforward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VR89wSInpLQ-"
   },
   "outputs": [],
   "source": [
    "dus_mm.form_output(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "splY6T0zpPON"
   },
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH3oUq9PoMO-"
   },
   "outputs": [],
   "source": [
    "# отрисовываем результат приближения нейросетью поверх исходной функции\n",
    "plt.scatter(X_val, y_val, color='black', antialiased=True)\n",
    "plt.plot(X_val, dus_mm.form_output(X_val,y_val), color='magenta', linewidth=2, antialiased=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se9YiqHNe0r0"
   },
   "outputs": [],
   "source": [
    "f = dus_mm.gradient_checking(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTD-6WNLtzzG"
   },
   "outputs": [],
   "source": [
    "lambda_vec = [0]\n",
    "error_train = np.zeros(len(lambda_vec))\n",
    "error_val = np.zeros(len(lambda_vec))\n",
    "init_theta = dus_mm.theta\n",
    "\n",
    "for i in range(len(lambda_vec)):\n",
    "  dus_mm.theta = init_theta.copy()\n",
    "  dus_mm.optimize(X_train, y_train, lambda_vec[i]) \n",
    "  a_out = dus_mm.feedforward(X_train)\n",
    "  a_end = a_out[:(dus_mm.sequential[-1].neurons * y_train.shape[0])]\n",
    "  error_train[i] = dus_mm.compute_loss(a_end, y_train)\n",
    "  \n",
    "  a_out = dus_mm.feedforward(X_val)\n",
    "  a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]\n",
    "  error_val[i] = dus_mm.compute_loss(a_end, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ooG5v7kwwDB"
   },
   "outputs": [],
   "source": [
    "plt.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\n",
    "plt.legend(['Train', 'Cross Validation'])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "print('lambda\\t\\tTrain Error\\tValidation Error')\n",
    "for i in range(len(lambda_vec)):\n",
    "    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-N-dl3ZgHUl"
   },
   "outputs": [],
   "source": [
    "a_out = dus_mm.feedforward(X_val)\n",
    "a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buMib2tFgHUm"
   },
   "outputs": [],
   "source": [
    "a_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9V2WwqAsUNG"
   },
   "outputs": [],
   "source": [
    "a_out = dus_mm.feedforward(X_val)\n",
    "a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]\n",
    "loss = dus_mm.compute_loss(a_end, y_val.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz0CxjXutF0U"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ds0ZnyYY2fQf"
   },
   "outputs": [],
   "source": [
    "y[1000:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfyMfEhagHUp"
   },
   "outputs": [],
   "source": [
    "X[1000:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi2eYwrJ6mqR"
   },
   "outputs": [],
   "source": [
    "np.round(dus_mm.feedforward(np.array(X[1000:1001]))[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d340da8"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('mnist.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "y[y == 10] = 0\n",
    "y0 = np.zeros((y.size, 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y0[i,y[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12bcac3f"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'loglos')\n",
    "dus_mm.add(Layer(128, 'sigmoid'))\n",
    "dus_mm.add(Layer(128, 'sigmoid'))\n",
    "dus_mm.add(Layer(y0.shape[1], 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V8Pu3j0awY0"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "lst =  np.arange(y.shape[0])\n",
    "np.random.shuffle(lst)\n",
    "split_data = int(0.8 * y.shape[0])\n",
    "\n",
    "X_train = X[lst[:split_data]]\n",
    "y_train = y[lst[:split_data]]\n",
    "X_val = X[lst[split_data:y.shape[0]]]\n",
    "y_val = y[lst[split_data:y.shape[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2474c593",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dus_mm.optimize(X_train, y_train, X_val, y_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "617622ac"
   },
   "outputs": [],
   "source": [
    "dus_mm.feedforward(X[600:601,:])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16463c06"
   },
   "outputs": [],
   "source": [
    "y0[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1567a10"
   },
   "outputs": [],
   "source": [
    " a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a70f9768"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
