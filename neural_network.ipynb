{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "91e9605d",
    "outputId": "5f0d650c-8db6-4346-e06c-15b8d63e4783"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#os.chdir('/content/drive/MyDrive/Github/dus_mm/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import glob\n",
    "from typing import List, Callable, Optional\n",
    "from functools import reduce\n",
    "os.chdir('data2')\n",
    "\n",
    "columnsX = ['freq','AX','BX','BY','CY','CX','DX','DY','AY']\n",
    "columnsY = ['AX','BX','BY','CY','CX','DX','DY','AY','AQ','AL','BQ','BL','CQ','CL','DQ','DL']\n",
    "columnsYout = ['AQ','AL','BQ','BL','CQ','CL','DQ','DL','X','Y','Q','L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1a3f18f9"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "numb_files = len(glob.glob('data_output*'))\n",
    "for i in glob.glob('data_output*'): \n",
    "    numb = i[11]\n",
    "    if (numb == 'data_output.txt'):\n",
    "        numb = ''\n",
    "    y = pd.concat([y,pd.read_table(os.path.join('data_output%s.txt' % numb), header = 0, names = columnsY, \n",
    "                                   index_col = False, sep='\\s+', engine='python', dtype ='float64')], ignore_index = 1)\n",
    "    X = pd.concat([X,pd.read_table(os.path.join('data_input%s.txt' % numb), header = 0, names = columnsX, \n",
    "                                   index_col = False, sep='\\s+', engine='python', dtype ='float64')], ignore_index = 1) \n",
    "nelem = 64\n",
    "freq_bias = [252.92, 97.13, 87.06, 57.57, 12.55, 3.40, -4.78, -11.92]\n",
    "#freq_bias = [9.8,   4.9,   1.03, -12.05]\n",
    "#freq_bias = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "X = utils.init_in(X, freq_bias)\n",
    "y = utils.decomposition(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y-0L1UGgHT6"
   },
   "outputs": [],
   "source": [
    "alfa = 1\n",
    "koef = np.zeros(len(freq_bias))\n",
    "X1 = utils.init_in(X.copy(), freq_bias)\n",
    "for i in range (100):\n",
    "    for n in range(len(freq_bias)):\n",
    "        koef[n] = (0.5 - X1['freq' + str(n+1)].mean())\n",
    "    freq_bias = freq_bias + koef * alfa\n",
    "    X1 = utils.init_in(X.copy(), freq_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction, y, diff: bool = False):\n",
    "#     if (loss_funtion == 'mean_squared_error'):\n",
    "    if (diff == True):\n",
    "        loss = prediction - y\n",
    "    else:\n",
    "        loss = np.sum(np.sum(np.power((prediction - y), 2), axis = None) )/ (2 * y.shape[0])\n",
    "#     elif (loss_funtion == 'loglos'):\n",
    "#         if (diff == True):\n",
    "#             loss = (prediction - y)/((1 - prediction) * prediction)\n",
    "#         else:\n",
    "#             loss = - np.sum(y * np.log(prediction) + (1 - y) * np.log(1 - prediction))/y.shape[0]     \n",
    "    return loss\n",
    "\n",
    "def activation(name: str, x: np.ndarray, diff: bool = False):\n",
    "        if (name == 'relu'):\n",
    "            if (diff == True):\n",
    "                x[x >= 0] = 1\n",
    "                x[x < 0] = 0\n",
    "            else:\n",
    "                x[x < 0] = 0\n",
    "        elif (name == 'linear'):\n",
    "            if (diff == True):\n",
    "                x = 1\n",
    "            else:\n",
    "                x = x\n",
    "        elif (name == 'sigmoid'):\n",
    "            if (diff == True):\n",
    "                x = activation(name, x) * (1 - activation(name, x))\n",
    "            else:\n",
    "                x = 1/(1 + np.exp(-x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "63666d57"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, previous_layer,  neurons: int, activation_name: str = 'linear') -> None:\n",
    "        self.previous_layer = previous_layer\n",
    "        self.next_layer = None\n",
    "        self.neurons: int = neurons\n",
    "        self.activation_name = activation_name\n",
    "        self.__weights: np.ndarray \n",
    "        self.weights_cache = self.initialization_weights()\n",
    "        self.outputs_cache: np.ndarray\n",
    "        self.deltas_cache: np.ndarray \n",
    "        self.gradients_cache: np.ndarray\n",
    "    \n",
    "    def initialization_weights(self) -> np.ndarray:\n",
    "        if self.previous_layer is None:\n",
    "            self.weights_cache = np.array([]) \n",
    "        else:\n",
    "            gamma = np.sqrt(6) / np.sqrt(self.previous_layer.neurons + self.neurons)\n",
    "            self.weights_cache = np.random.rand(self.neurons,self.previous_layer.neurons + 1) * 2 * gamma - gamma          \n",
    "            return self.weights_cache\n",
    "    \n",
    "    def calculate_outputs(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        if self.previous_layer is None:\n",
    "            self.outputs_cache = inputs\n",
    "        else:\n",
    "            inputs = np.concatenate((np.ones([inputs.shape[0],1]), inputs), axis = 1)\n",
    "            self.outputs_cache =  activation(self.activation_name, inputs @ self.weights_cache.T)       \n",
    "        return self.outputs_cache\n",
    "    \n",
    "    def calculate_deltas_and_gradients(self, y: np.ndarray) -> None:\n",
    "        inputs = self.previous_layer.outputs_cache\n",
    "        inputs = np.concatenate((np.ones([inputs.shape[0],1]), inputs), axis = 1)\n",
    "        if self.next_layer is None:\n",
    "            self.deltas_cache = compute_loss(self.outputs_cache, y, diff = True) * activation(self.activation_name, np.dot(inputs, self.weights_cache.T) , diff = True)\n",
    "        else:\n",
    "            self.deltas_cache = np.dot(self.next_layer.deltas_cache, self.next_layer.weights_cache[:,1:]) * activation(self.activation_name, np.dot(inputs, self.weights_cache.T), diff = True)\n",
    "        self.gradients_cache = (1 / y.shape[0]) * np.dot(self.deltas_cache.T, inputs)\n",
    "    \n",
    "    def _record_weights(self):\n",
    "        self.__weights = self.weights_cache\n",
    "    \n",
    "    def _playback_weights(self):\n",
    "        self.weights_cache = self.__weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e26a390"
   },
   "outputs": [],
   "source": [
    "class NN_model: \n",
    "    def __init__(self, input_dimension: int, loss = 'mean_squared_error'):\n",
    "        self.sum_neurons = 0\n",
    "        self.layers: List[Layer] = [] \n",
    "        self.add_layer(input_dimension)\n",
    "        self.loss = loss\n",
    "        self.optimizer = 'adam' #'adam'\n",
    "        self.max_epoch = 1000\n",
    "        self.chunk_size = 32\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99\n",
    "        self.epsilon = 1e-8\n",
    "        self.learning_rate = 0.01\n",
    "        self.regularization = None #'l2'\n",
    "        self.best_loss_val = 1e10\n",
    "    \n",
    "    def add_layer(self, neurons: int, activation) -> None:\n",
    "        if not self.layers:\n",
    "            new_layer = Layer(None, neurons)\n",
    "        else:\n",
    "            new_layer = Layer(self.layers[-1], neurons, activation)\n",
    "            self.layers[-1].next_layer = new_layer   \n",
    "        self.layers.append(new_layer)\n",
    "        self.sum_neurons += neurons \n",
    "    \n",
    "    def calculate_outputs(self, inputs:  np.ndarray) -> np.ndarray:\n",
    "         return reduce(lambda inputs, layer: layer.calculate_outputs(inputs), self.layers, inputs)  \n",
    "    \n",
    "    def backpropagate(self, y):\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            self.layers[i].calculate_deltas_and_gradients(y)          \n",
    "\n",
    "    def gradient_checking(self, x, y) -> None:\n",
    "        e: float = 1e-4\n",
    "        numgrad = np.array([])\n",
    "        grad = np.array([])\n",
    "        self.calculate_outputs(x)\n",
    "        self.backpropagate(y)\n",
    "        j = 0\n",
    "        for i in range(1, len(self.layers)):\n",
    "            grad = np.concatenate([grad, self.layers[i].gradients_cache.ravel()])\n",
    "            numgrad = np.concatenate([numgrad, np.zeros(self.layers[i].weights.size)])\n",
    "            perturb = np.diag(e * np.ones(self.layers[i].weights.size))\n",
    "            weights = self.layers[i].weights.copy()\n",
    "            for k in range(self.layers[i].weights.size):\n",
    "                self.layers[i].weights += perturb[:,k].reshape(self.layers[i].weights.shape)\n",
    "                predicted_plus  = self.calculate_outputs(x)\n",
    "                self.layers[i].weights -= perturb[:,k].reshape(self.layers[i].weights.shape)\n",
    "                predicted_minus = self.calculate_outputs(x)\n",
    "                numgrad[j] = (compute_loss(predicted_plus, y) - compute_loss(predicted_minus, y))/(2 * e)\n",
    "                j += 1\n",
    "            self.layers[i]._playback_weights(self):\n",
    "        diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "        print('Relative Difference: %g' % diff) \n",
    "     \n",
    "#     def optimize(self, X_train, y_train, X_val, y_val, lambda_new = 0 , theta_new = None):\n",
    "#         if (theta_new is None):\n",
    "#             theta_new = self.theta\n",
    "#         for epoch in range(self.max_epoch):\n",
    "#             m = np.zeros(self.theta.shape)\n",
    "#             v = np.zeros(self.theta.shape)\n",
    "#             lst =  np.arange(y_train.shape[0])\n",
    "#             np.random.shuffle(lst)\n",
    "#             chunks = np.array([lst[i: i + self.chunk_size] for i in range(0, y_train.shape[0], self.chunk_size)])\n",
    "#             for t, chunk in enumerate(chunks):\n",
    "#                 xi = X_train[chunk]\n",
    "#                 yi = y_train[chunk]\n",
    "#                 self.calculate_outputs(xi)\n",
    "#                 self.backpropagate(yi)\n",
    "#                 if (self.optimizer == 'adam'):\n",
    "#                     for i in range(1, len(self.layers)):\n",
    "#                         m = self.beta_1 * m + (1 - self.beta_1) * layer[i].calculate_gradients()\n",
    "#                         v = self.beta_2 * v + (1 - self.beta_2) * np.power(grad, 2)\n",
    "#                         m_hat = m / (1 - np.power(self.beta_1, t + 1))\n",
    "#                         v_hat = v / (1 - np.power(self.beta_2, t + 1))\n",
    "#                         layers[i].weights = layers[i].weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon) \n",
    "#                 elif (self.optimizer == 'gradient_descent'):\n",
    "#                     theta_new = theta_new - self.learning_rate * grad\n",
    "#             a_out_train = self.feedforward(X_train, theta_new)\n",
    "  \n",
    "#             loss_train = self.compute_loss(a_out_train, y_train, theta_new)# + lambda_new * np.sum(np.power(self.nulling(theta_new), 2))/2\n",
    "#             a_out_val = self.feedforward(X_val, theta_new)\n",
    "#             loss_val = self.compute_loss(a_out_val, y_val, theta_new) #+ lambda_new * np.sum(np.power(self.nulling(theta_new), 2))/2\n",
    "            \n",
    "#             if (self.best_loss_val > loss_val):  \n",
    "#                 self.theta = theta_new\n",
    "#                 self.best_loss_val = loss_val        \n",
    "                \n",
    "#             print(f'{np.round(loss_train,2)}    {np.round(loss_val,2)}    {epoch}')\n",
    "#         return loss_train, loss_val\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return str(len(self.sequential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGYK1b3NR6-q"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'loglos')\n",
    "dus_mm.add_layer(16, 'sigmoid')\n",
    "dus_mm.add_layer(16, 'sigmoid')\n",
    "dus_mm.add_layer(y.shape[1], 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.calculate_outputs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.backpropagate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dus_mm.gradient_checking(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dus_mm.layers[2].output_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh_MQJ_MR3Hp"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('mnist.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "y[y == 10] = 0\n",
    "y0 = np.zeros((y.size, 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y0[i,y[i]] = 1\n",
    "y = y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa38a8e6"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1])\n",
    "dus_mm.add(Layer(128, 'relu'))\n",
    "dus_mm.add(Layer(128, 'relu'))\n",
    "dus_mm.add(Layer(y.shape[1], 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFpD5CtzZlxA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# накидываем тысячу точек от -3 до 3\n",
    "X = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "\n",
    "# задаём линейную функцию, которую попробуем приблизить нашей нейронной сетью\n",
    "def f(x):    \n",
    "    return 2 * np.sin(x) + 5\n",
    "\n",
    "f = np.vectorize(f)\n",
    "\n",
    "# вычисляем вектор значений функции\n",
    "y = f(X)\n",
    "\n",
    "# отрисовываем результат приближения нейросетью поверх исходной функции\n",
    "plt.scatter(X_train, y, color='black', antialiased=True)\n",
    "#plt.plot(, model.predict(x), color='magenta', linewidth=2, antialiased=True)\n",
    "plt.show()\n",
    "\n",
    "# выводим веса на экран\n",
    "# for layer in model.layers:\n",
    "#     weights = layer.get_weights()\n",
    "#     print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XlZMY-XBaHP5"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'mean_squared_error')\n",
    "dus_mm.add(Layer(y.shape[1], 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1624464853613,
     "user": {
      "displayName": "Maxim Tabolin",
      "photoUrl": "",
      "userId": "11591994697218843812"
     },
     "user_tz": -180
    },
    "id": "jOpGhdRhNZe4",
    "outputId": "f4c275cd-736e-4c7c-d5bb-a7c15818c4e2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "lambda_vec = [0]\n",
    "\n",
    "nfold = 5\n",
    "slice_fold = int(y.shape[0] // nfold)\n",
    "lst =  np.arange(y.shape[:(5 * slice_fold)][0])\n",
    "np.random.shuffle(lst)\n",
    "list_fold = np.array([lst[i * slice_fold:(i + 1) * slice_fold] for i in range(nfold)])\n",
    "\n",
    "train_error = np.zeros(nfold)\n",
    "val_error = np.zeros(nfold)\n",
    "train_reg = np.zeros(len(lambda_vec))\n",
    "val_reg = np.zeros(len(lambda_vec))\n",
    "theta_new = dus_mm.theta.copy()\n",
    "\n",
    "for k in range(len(lambda_vec)):\n",
    "    theta_new\n",
    "    for i in range(nfold):\n",
    "        X_train = X[np.delete(list_fold, i, 0).ravel()]\n",
    "        y_train = y[np.delete(list_fold, i, 0).ravel()]\n",
    "        X_val = X[list_fold[i]]\n",
    "        y_val = y[list_fold[i]]\n",
    "        train_error[i], val_error[i] = dus_mm.optimize(X_train, y_train, X_val, y_val, lambda_vec[k], theta_new)\n",
    "    train_reg[k] = np.mean(train_error)  \n",
    "    val_reg[k] = np.mean(val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S7sRgeqgHUZ"
   },
   "source": [
    "['AQ','AL','BQ','BL','CQ','CL','DQ','DL','X','Y','Q','L']\n",
    "  -1., -0., -2.,  1., 34., 65.,  5., 23., 81., -2., 18., 21\n",
    "   2.,  1.,  1.,  0., 15., 65., 24.,  2., 83.,  1.,  7.,  4.\n",
    "   0.,  1., -1.,  10.,21., 63., 26.,  6., 86.,  -1., 12.,-20.\n",
    "      \n",
    "   1.,  1.,  3.,  7., 47., 15., 20.,  48., 54.,  3.,-11., 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqd2hAFPgHUa"
   },
   "outputs": [],
   "source": [
    "XX = pd.read_table(os.path.join('data7A-1_2.txt'), header = 0, names = columnsX, index_col = False, sep='\\s+', engine='python', dtype =\"float\")\n",
    "XX = utils.init_in(XX,freq_bias)\n",
    "yy = np.round(dus_mm.feedforward(np.array(XX))[:12])\n",
    "#yy = np.abs(pd.DataFrame(yy.astype('float'), columns = columnsYout))\n",
    "#yy = utils.integration(yy,nelem)\n",
    "# predictions = NN_model.predict(XX)\n",
    "# yy = np.abs(pd.DataFrame(predictions.astype('float'), columns = columnsYout))\n",
    "#yy = utils.integration(yy,0.72*1.33,1.33) # перемычки 30 мкм вместо 40 мкм 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB-t23aygHUc",
    "outputId": "c4f73a2e-a70c-4d9f-b05a-b5ea30d0823a"
   },
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF3fAeAjgHUd"
   },
   "outputs": [],
   "source": [
    "len(columnsYout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-MlF9J-gHUe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(yy.astype('float'), col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN3uHI8NgHUf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yy = utils.integration(np.round(dus_mm.feedforward(np.array(XX))[:12]),nmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eto14XwfoaCJ"
   },
   "outputs": [],
   "source": [
    "a_out_train = self.feedforward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VR89wSInpLQ-"
   },
   "outputs": [],
   "source": [
    "dus_mm.form_output(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "splY6T0zpPON"
   },
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH3oUq9PoMO-"
   },
   "outputs": [],
   "source": [
    "# отрисовываем результат приближения нейросетью поверх исходной функции\n",
    "plt.scatter(X_val, y_val, color='black', antialiased=True)\n",
    "plt.plot(X_val, dus_mm.form_output(X_val,y_val), color='magenta', linewidth=2, antialiased=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se9YiqHNe0r0"
   },
   "outputs": [],
   "source": [
    "f = dus_mm.gradient_checking(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTD-6WNLtzzG"
   },
   "outputs": [],
   "source": [
    "lambda_vec = [0]\n",
    "error_train = np.zeros(len(lambda_vec))\n",
    "error_val = np.zeros(len(lambda_vec))\n",
    "init_theta = dus_mm.theta\n",
    "\n",
    "for i in range(len(lambda_vec)):\n",
    "  dus_mm.theta = init_theta.copy()\n",
    "  dus_mm.optimize(X_train, y_train, lambda_vec[i]) \n",
    "  a_out = dus_mm.feedforward(X_train)\n",
    "  a_end = a_out[:(dus_mm.sequential[-1].neurons * y_train.shape[0])]\n",
    "  error_train[i] = dus_mm.compute_loss(a_end, y_train)\n",
    "  \n",
    "  a_out = dus_mm.feedforward(X_val)\n",
    "  a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]\n",
    "  error_val[i] = dus_mm.compute_loss(a_end, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ooG5v7kwwDB"
   },
   "outputs": [],
   "source": [
    "plt.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\n",
    "plt.legend(['Train', 'Cross Validation'])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "print('lambda\\t\\tTrain Error\\tValidation Error')\n",
    "for i in range(len(lambda_vec)):\n",
    "    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-N-dl3ZgHUl"
   },
   "outputs": [],
   "source": [
    "a_out = dus_mm.feedforward(X_val)\n",
    "a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buMib2tFgHUm"
   },
   "outputs": [],
   "source": [
    "a_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9V2WwqAsUNG"
   },
   "outputs": [],
   "source": [
    "a_out = dus_mm.feedforward(X_val)\n",
    "a_end = a_out[:(dus_mm.sequential[-1].neurons * y_val.shape[0])]\n",
    "loss = dus_mm.compute_loss(a_end, y_val.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz0CxjXutF0U"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ds0ZnyYY2fQf"
   },
   "outputs": [],
   "source": [
    "y[1000:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfyMfEhagHUp"
   },
   "outputs": [],
   "source": [
    "X[1000:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi2eYwrJ6mqR"
   },
   "outputs": [],
   "source": [
    "np.round(dus_mm.feedforward(np.array(X[1000:1001]))[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d340da8"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('mnist.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "y[y == 10] = 0\n",
    "y0 = np.zeros((y.size, 10))\n",
    "for i in range(y.shape[0]):\n",
    "    y0[i,y[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12bcac3f"
   },
   "outputs": [],
   "source": [
    "dus_mm = NN_model(X.shape[1], 'loglos')\n",
    "dus_mm.add(Layer(128, 'sigmoid'))\n",
    "dus_mm.add(Layer(128, 'sigmoid'))\n",
    "dus_mm.add(Layer(y0.shape[1], 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V8Pu3j0awY0"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "lst =  np.arange(y.shape[0])\n",
    "np.random.shuffle(lst)\n",
    "split_data = int(0.8 * y.shape[0])\n",
    "\n",
    "X_train = X[lst[:split_data]]\n",
    "y_train = y[lst[:split_data]]\n",
    "X_val = X[lst[split_data:y.shape[0]]]\n",
    "y_val = y[lst[split_data:y.shape[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2474c593",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dus_mm.optimize(X_train, y_train, X_val, y_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "617622ac"
   },
   "outputs": [],
   "source": [
    "dus_mm.feedforward(X[600:601,:])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16463c06"
   },
   "outputs": [],
   "source": [
    "y0[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1567a10"
   },
   "outputs": [],
   "source": [
    " a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a70f9768"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
